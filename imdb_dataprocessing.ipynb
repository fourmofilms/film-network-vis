{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bc49b4-d7db-4686-9da8-138d974484cb",
   "metadata": {},
   "source": [
    "standard imports here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da2386f-4eef-48d3-8557-386656ca1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37671882-777c-409a-b098-dd6c9aa440c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_path = os.path\n",
    "csv_writer = csv.writer\n",
    "sys_exit = sys.exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58928bf0-9b17-4229-a697-3d16a5591b90",
   "metadata": {},
   "source": [
    "many names (of people and films) have special characters in them. most can be handled by encoding the resulting files in ANSI, but some of them still pose an issue. here, i'm just replacing them so everything writes well.\n",
    "the function returns the row (with replaces characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79055679-58cf-462c-86f3-14059ca3a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_replace(row):\n",
    "    for i in range(len(row)):\n",
    "        row[i] = row[i].replace(\"ā\", \"a\").replace(\"ğ\", \"g\").replace(\"ė\", \"e\")\n",
    "        row[i] = row[i].replace(\"ō\", \"o\").replace(\"ē\", \"e\").replace(\"ʽ\", \"'\")\n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a85efa-541f-40bf-9934-4c5555403b68",
   "metadata": {},
   "source": [
    "this is just so it's easy to find the average weighted rating of a group of films. only used to find 'imdb rating' for people, but could have other uses.\n",
    "the function returns the average imdb score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7cf651-4bde-462d-9408-891bf6a80078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_rating(title_list):\n",
    "    weighted = 0\n",
    "    samp = 0\n",
    "    score = 999\n",
    "    for i in title_list:\n",
    "        try:\n",
    "            weighted += (float(rating_dict[i][0][1]) * float(rating_dict[i][0][2]))\n",
    "        except KeyError:\n",
    "            weighted += 0\n",
    "        try:\n",
    "            samp += float(rating_dict[i][0][2])\n",
    "        except KeyError:\n",
    "            score = ''\n",
    "        \n",
    "    if score != '':\n",
    "        score = weighted / samp\n",
    "    \n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc854c-1eca-471c-b1ca-edb6859fae5d",
   "metadata": {},
   "source": [
    "this is what converts the raw imdb files into stored dictionaries. this takes the imdb file and a list of codes to include. it places rows into a dictionary, ignoring those that aren't in the 'inc_list'. no real processing is done here, this is just to slim down the csv files.\n",
    "the function prints how long it took to process the csv.\n",
    "the function returns the dictionary it created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8ac95f-f4b2-4110-a991-36a0be9170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gztsv_to_dict(file_name, inc_list, to_write, write_file):\n",
    "    dict_name = {}\n",
    "    multi_row_list = []\n",
    "    o_timestam = datetime.now()\n",
    "    print(o_timestam)\n",
    "    times = []\n",
    "    with gzip.open(file_name, 'rt', encoding='utf-8') as tsv_file:\n",
    "        reader = csv.reader(tsv_file, delimiter='\\t', quotechar='\\'', escapechar='|')\n",
    "        for row in reader:\n",
    "            if row[0] in inc_list:\n",
    "                timestam = datetime.now()\n",
    "                row = letter_replace(row)\n",
    "                if row[0] not in multi_row_list:\n",
    "                    dict_name[row[0]] = [row]\n",
    "                    multi_row_list.append(row[0])\n",
    "                else:\n",
    "                    dict_name[row[0]].append(row)\n",
    "                times.append(datetime.now() - timestam)\n",
    "    \n",
    "    if to_write == 'y':\n",
    "        dict_to_csv(write_file, dict_name)\n",
    "        \n",
    "    \n",
    "    print('max processing time: ', max(times))\n",
    "    #print('min processing time: ', min(times))\n",
    "    print('total processing time: ', datetime.now() - o_timestam)\n",
    "    print(datetime.now())\n",
    "    tsv_file.close()\n",
    "    return (dict_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84c57a-b055-457f-b096-982d8aca1fa1",
   "metadata": {},
   "source": [
    "this converts a dictionary to a new csv file. again, no processing.\n",
    "the function returns nothing, but a new file is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef1e9c3-68f9-46c6-aec3-3404ba1c8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_csv(file_name, dict_name):\n",
    "    csv_file = open(file_name, 'w', newline='', encoding='ANSI')\n",
    "    writer = csv_writer(csv_file, delimiter=',', quotechar=None,escapechar=' ')\n",
    "\n",
    "    for key in dict_name:\n",
    "        for i in range(0,len(dict_name[key])):\n",
    "            writer.writerow(dict_name[key][i])\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166cbdc2-1797-4727-9ec7-c3ac7e1c5e25",
   "metadata": {},
   "source": [
    "this is just getting the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b07bc7-3e47-4f4e-bac9-afdd88435290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input path to watched film .tsv or .tab file:  watched_film_data.tsv\n"
     ]
    }
   ],
   "source": [
    "watched_file_path = input('Input path to watched film .tsv or .tab file: ')\n",
    "#watched_file_path = 'watched_film_data.tsv'\n",
    "if not os_path.isfile(watched_file_path):\n",
    "    print('file not found :(')\n",
    "    sys_exit()\n",
    "\n",
    "if not watched_file_path.endswith('.tsv') and watched_file_path.endswith('.tab'):\n",
    "    print('you must input path to .tsv or .tab file')\n",
    "    sys_exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f7ea4b-329b-4288-baba-e5b1f4a73792",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = 'title.basics.tsv.gz'\n",
    "principles = 'title.principals.tsv.gz'\n",
    "crew = 'title.crew.tsv.gz'\n",
    "names = 'name.basics.tsv.gz'\n",
    "ratings = 'title.ratings.tsv.gz'\n",
    "\n",
    "if not os_path.isfile(titles):\n",
    "    print('titles file not found :(')\n",
    "    sys_exit()\n",
    "if not os_path.isfile(principles):\n",
    "    print('principles file not found :(')\n",
    "    sys_exit()\n",
    "if not os_path.isfile(crew):\n",
    "    print('crew file not found :(')\n",
    "    sys_exit()\n",
    "if not os_path.isfile(names):\n",
    "    print('names file not found :(')\n",
    "    sys_exit()\n",
    "if not os_path.isfile(ratings):\n",
    "    print('ratings file not found :(')\n",
    "    sys_exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b4e75a2-5530-4e50-a869-558f192de349",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_titles = 'cut_titles.csv'\n",
    "cut_principles = 'cut_principles.csv'\n",
    "cut_crew = 'cut_crew.csv'\n",
    "cut_names = 'cut_names.csv'\n",
    "cut_ratings = 'cut_ratings.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33af248-c669-4429-8217-3dbcce6461a7",
   "metadata": {},
   "source": [
    "asking if the file should produce a 'processed' version of all the files (essentially, a version of the original file with just the films/people in the 'included' lists.\n",
    "\n",
    "also asking if the vertex names(plot) should be the official primary titles (with year) or the ones in the custom file. this should probably be the official unless every one of your titles is unique (or if the you don't want the english titles).\n",
    "\n",
    "can bypass this with 'default' settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "915f2287-1570-42fe-9319-7f1aedd6f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want default settings [~produce all cut files, official vertex names] (y/n):  y\n"
     ]
    }
   ],
   "source": [
    "wrong_letter = ('you must enter y or n')\n",
    "\n",
    "default = input('do you want default settings [~produce all cut files, official vertex names] (y/n): ').lower()\n",
    "if default == 'y':\n",
    "    all_check_in = 'y'\n",
    "elif default not in ('y','n'):\n",
    "    print(wrong_letter)\n",
    "    sys_exit()\n",
    "else:\n",
    "    all_check_in = input('do you want all the cut files? yes, some, or none?(y/s/n): ').lower()\n",
    "\n",
    "if all_check_in in ('y','n'):\n",
    "    title_check = all_check_in\n",
    "    principles_check = all_check_in\n",
    "    crew_check = all_check_in\n",
    "    name_check = all_check_in\n",
    "    rating_check = all_check_in\n",
    "\n",
    "elif all_check_in == 's':\n",
    "\n",
    "    title_check = input('do you want a cut title basics file? (y/n): ').lower()\n",
    "    if title_check not in ('y', 'n'):\n",
    "        print(wrong_letter)\n",
    "        sys_exit()\n",
    "\n",
    "    principles_check = input('do you want a cut title principles file? (y/n): ').lower()\n",
    "    if principles_check not in ('y', 'n'):\n",
    "        print(wrong_letter)\n",
    "        sys_exit()\n",
    "    crew_check = input('do you want a cut title crew file? (y/n): ').lower()\n",
    "    if crew_check not in ('y', 'n'):\n",
    "        print('you must enter y or n')\n",
    "        sys_exit()\n",
    "\n",
    "    name_check = input('do you want a cut name basics file? (y/n): ').lower()\n",
    "    if name_check not in ('y', 'n'):\n",
    "        print(wrong_letter)\n",
    "        sys_exit()\n",
    "\n",
    "    rating_check = input('do you want a cut title ratings file? (y/n): ').lower()\n",
    "    if rating_check not in ('y', 'n'):\n",
    "        print(wrong_letter)\n",
    "        sys_exit()\n",
    "\n",
    "else:\n",
    "    print('you must enter y, s, or n')\n",
    "    sys_exit\n",
    "\n",
    "\n",
    "if default == 'y':\n",
    "    name_op = 'o'\n",
    "else:\n",
    "    name_op = input(\"do you want the vertex (film titles) names to be imdb's official titles or yours? (o/y): \").lower()\n",
    "    if name_op not in ('o', 'y'):\n",
    "        print('you must enter o or y')\n",
    "        sys_exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ecea25-13f8-4694-b676-dcb58c92516a",
   "metadata": {},
   "source": [
    "this is file processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5d0aa-3660-45ad-a474-6633a00b293c",
   "metadata": {},
   "source": [
    "starting with watched films. this reads the tsv, processing info, and created the list of watched films (codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2286a798-a953-4ab8-84b1-38fc14408c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 16:48:34.087242\n",
      "2022-07-02 16:48:34.098232\n",
      "collected watched films. time:  0:00:00.010990\n"
     ]
    }
   ],
   "source": [
    "#start with custom file\n",
    "watched_dict = {}\n",
    "watched_codes = []\n",
    "rating_list = []\n",
    "o_timestam = datetime.now()\n",
    "print(o_timestam)\n",
    "with open(watched_file_path, 'r', newline='') as watched_tsv_file:\n",
    "\n",
    "    reader = csv.reader(watched_tsv_file, delimiter='\\t', quotechar='\\'')\n",
    "    next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        \n",
    "        code = row[0]\n",
    "        watched_date = datetime.strptime(row[1], '%m/%d/%Y')\n",
    "        watched_date = datetime.date(watched_date)\n",
    "        title = row[2]\n",
    "        seen = row[3]\n",
    "        source = row[4]\n",
    "        recc = row[5]\n",
    "        rating = row[6]\n",
    "        simple = row[7]\n",
    "        \n",
    "        if code != 'NA':\n",
    "            watched_codes.append(code)\n",
    "            rating_list.append(code)\n",
    "        \n",
    "        watched_dict[code] = [code, watched_date, title, seen, source, recc, rating, simple]\n",
    "        \n",
    "#done with this\n",
    "print(datetime.now())\n",
    "print('collected watched films. time: ', datetime.now() - o_timestam)\n",
    "watched_tsv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96dcbc-a103-49bd-a0d8-58a0569baed9",
   "metadata": {},
   "source": [
    "these three files can be read with just the watched films list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae57761-f71c-4ff5-989d-a5ef1d7f80d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 16:48:34.120163\n",
      "max processing time:  0:00:00.001009\n",
      "total processing time:  0:01:22.409795\n",
      "2022-07-02 16:49:56.529958\n",
      "titles done\n",
      "2022-07-02 16:49:56.529958\n",
      "max processing time:  0:00:00.015622\n",
      "total processing time:  0:06:56.922632\n",
      "2022-07-02 16:56:53.452590\n",
      "principles done\n",
      "2022-07-02 16:56:53.453587\n",
      "max processing time:  0:00:00.000997\n",
      "total processing time:  0:01:10.659391\n",
      "2022-07-02 16:58:04.112978\n",
      "crew done\n"
     ]
    }
   ],
   "source": [
    "title_dict = gztsv_to_dict(titles, watched_codes, title_check, cut_titles)\n",
    "print('titles done')\n",
    "principle_dict = gztsv_to_dict(principles, watched_codes, principles_check, cut_principles)\n",
    "print('principles done')\n",
    "crew_dict = gztsv_to_dict(crew, watched_codes, crew_check, cut_crew)\n",
    "print('crew done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5501ffd-60f8-4035-9228-18f267361247",
   "metadata": {},
   "source": [
    "a little more processing has to be done before the people/rating processing can be done. this creates a list of people involved with the films (principles and crews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5a5079-6a42-4c1d-8c63-3434af441a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_codes = []\n",
    "connection_list = []\n",
    "for code in principle_dict:\n",
    "    for i in range(0,len(principle_dict[code])):\n",
    "        person = principle_dict[code][i][2]\n",
    "        job = principle_dict[code][i][3]\n",
    "        cat = principle_dict[code][i][4]\n",
    "        if job != \"\\\\N\":\n",
    "            role = job\n",
    "        else:\n",
    "            role = cat\n",
    "    \n",
    "        role = principle_dict[code][i][3]\n",
    "        connection = [code, person, role]\n",
    "        if connection not in connection_list:\n",
    "            connection_list.append(connection)\n",
    "        people_codes.append(person)\n",
    "\n",
    "for code in crew_dict:\n",
    "    directors = crew_dict[code][0][1]\n",
    "    writers = crew_dict[code][0][2]\n",
    "    dir_list = directors.split(',')\n",
    "    wri_list = writers.split(',')\n",
    "    for person in dir_list:\n",
    "        connection = [code, person, 'director']\n",
    "        if connection not in connection_list:\n",
    "            connection_list.append(connection)\n",
    "        if person not in people_codes:\n",
    "            people_codes.append(person)\n",
    "    for person in dir_list:\n",
    "        connection = [code, person, 'writer']\n",
    "        if connection not in connection_list:\n",
    "            connection_list.append(connection)\n",
    "        if person not in people_codes:\n",
    "            people_codes.append(person)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719c792-c30d-44b6-967c-7e448aaf49bc",
   "metadata": {},
   "source": [
    "then the people/names file can be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aac6f7f-b48e-4805-85d2-4e9817d2a1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 16:58:04.489578\n",
      "max processing time:  0:00:00.015623\n",
      "total processing time:  0:14:19.196844\n",
      "2022-07-02 17:12:23.686422\n",
      "names done\n"
     ]
    }
   ],
   "source": [
    "people_dict = gztsv_to_dict(names, people_codes, name_check, cut_names)\n",
    "print('names done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e2c80-f3cc-482e-b956-a8e0b3e1cd92",
   "metadata": {},
   "source": [
    "people's 'known films' have be to included when processing the ratings, so we can get the average ratings for people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "673e5f4a-aad0-4712-b50f-bdea8f5fd11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in people_dict:\n",
    "    films = (people_dict[person][0][5]).split(',')\n",
    "    for code in films:\n",
    "        rating_list.append(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45884015-bc32-4ccc-8365-ee1d689e6cc5",
   "metadata": {},
   "source": [
    "then the final file (ratings) can be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edd67ec3-a1fe-479c-8aaf-9059fae1aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 17:12:23.717632\n",
      "max processing time:  0:00:00.015656\n",
      "total processing time:  0:04:30.150115\n",
      "2022-07-02 17:16:53.867747\n",
      "ratings done\n"
     ]
    }
   ],
   "source": [
    "rating_dict = gztsv_to_dict(ratings, rating_list, rating_check, cut_ratings)\n",
    "print('ratings done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817c83c-43d6-4f8f-9f90-c7af54dcd2ad",
   "metadata": {},
   "source": [
    "this creates a dictionary of films, including the needed processing. this is essentially what's included in the vertex file. processing includes adding year to title names (for unique-ness) and breaking out the genres into three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14213c1a-25a8-46b7-9337-d7d4561a7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "film_details = {}\n",
    "for code in watched_codes:\n",
    "    title = watched_dict[code][2]\n",
    "    official = title_dict[code][0][2] + '(' + title_dict[code][0][5] + ')'\n",
    "    watched = watched_dict[code][1]\n",
    "    ttype = title_dict[code][0][1]\n",
    "    year = int(title_dict[code][0][5])\n",
    "    decade = year//10 * 10\n",
    "    half_decade = (((year - decade) // 5) * 5) + decade\n",
    "    age = (date.today().year - year)\n",
    "    runtime = int(title_dict[code][0][7])\n",
    "    genres = title_dict[code][0][8].split(',')\n",
    "    genres.extend(['',''])\n",
    "    genre1 = genres[0]\n",
    "    genre2 = genres[1]\n",
    "    genre3 = genres[2]\n",
    "    seen = watched_dict[code][3]\n",
    "    source = watched_dict[code][4]\n",
    "    simple = float(watched_dict[code][7])\n",
    "    imdb_rating = rating_dict[code][0][1]\n",
    "    recc = watched_dict[code][5]\n",
    "    film_details[code] = [code,\n",
    "                          title,official,watched,ttype,year,\n",
    "                          decade,half_decade,age,runtime,genre1,\n",
    "                          genre2,genre3,seen,source,simple,\n",
    "                          imdb_rating,recc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870676e-0fb7-4624-bf31-b0dde858360f",
   "metadata": {},
   "source": [
    "this creates a dictionary of people, including the needed processing. this is essentially what's included in the vertex file. processing includes adding the last 4 of the unique id to people names (for unique-ness). pretty much the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c22b8f-eda3-4455-ad8f-f6b9abf93bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_details = {}\n",
    "for code in people_codes:\n",
    "    name = people_dict[code][0][1]\n",
    "    official = people_dict[code][0][1] + '(' + code[-4:] + ')'\n",
    "    watched = ''\n",
    "    profs = (people_dict[code][0][4]).split(',')\n",
    "    profs.extend(['','',''])\n",
    "    ttype = profs[0]\n",
    "    try:\n",
    "        year = int(people_dict[code][0][2])\n",
    "        decade = year//10 * 10\n",
    "        half_decade = (((year - decade) // 5) * 5) + decade\n",
    "        age = (date.today().year - year)\n",
    "    except:\n",
    "        year = ''\n",
    "        decade = ''\n",
    "        half_decade = ''\n",
    "        age = ''\n",
    "    runtime = ''\n",
    "    prof1 = profs[0]\n",
    "    prof2 = profs[1]\n",
    "    prof3 = profs[2]\n",
    "    seen = ''\n",
    "    source = ''\n",
    "    simple = ''\n",
    "    imdb_rating = avg_rating((people_dict[code][0][5]).split())\n",
    "    recc = ''\n",
    "    people_details[code] = [code,\n",
    "                          name,official,watched,ttype,year,\n",
    "                          decade,half_decade,age,runtime,prof1,\n",
    "                          prof2,prof3,seen,source,simple,\n",
    "                          imdb_rating,recc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872efb6f-791b-419d-b778-070c3491033e",
   "metadata": {},
   "source": [
    "these actually create the final files (edges and vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88bb1cd5-747a-4b07-bcd1-d0ca497636a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 17:16:53.945910\n",
      "2022-07-02 17:16:53.961443\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "edges = open('edges.csv', 'w', newline='', encoding='ANSI')\n",
    "writer = csv_writer(edges, delimiter=',', quotechar='\"',quoting=csv.QUOTE_ALL,escapechar=' ')\n",
    "\n",
    "writer.writerow(['title','name','role'])\n",
    "\n",
    "for pair in connection_list:\n",
    "    \n",
    "    if connection_list[1] != '\\\\N':\n",
    "\n",
    "        if name_op == 'o':\n",
    "            i = 2\n",
    "            k = (2,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17)\n",
    "            pk = (2,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17)\n",
    "        else:\n",
    "            i = 1\n",
    "            k = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17)\n",
    "            pk = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17)\n",
    "\n",
    "        pair[2] = pair[2].replace('\"', \"'\")\n",
    "\n",
    "        new_pair = [film_details[pair[0]][i],people_details[pair[1]][2],pair[2]]\n",
    "\n",
    "        writer.writerow(new_pair)\n",
    "\n",
    "print(datetime.now())\n",
    "edges.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d0ab6b-f284-4199-a5a3-1637a46f64cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-02 17:16:54.008309\n",
      "all done ...\n"
     ]
    }
   ],
   "source": [
    "vert = open('vertices.csv', 'w', newline='', encoding='ANSI')\n",
    "writer = csv_writer(vert, delimiter=',', quotechar='\"',quoting=csv.QUOTE_ALL,escapechar=' ')\n",
    "\n",
    "writer.writerow(['official_title','your_title','watch_date','type','year','decade','half_decade','age','runtime','genre1','genre2','genre3','seen','source','rating','imdb_rating','recommender'])\n",
    "\n",
    "for key in film_details:\n",
    "    film_row = []\n",
    "    for thing in k:\n",
    "        film_row.append(film_details[key][thing])\n",
    "    writer.writerow(film_row)\n",
    "\n",
    "for key in people_details:\n",
    "    person_row = []\n",
    "    for thing in pk:\n",
    "        person_row.append(people_details[key][thing])\n",
    "    writer.writerow(person_row)\n",
    "\n",
    "print(datetime.now())\n",
    "print('all done ...')\n",
    "vert.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
